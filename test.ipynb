{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53cc6e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5503def",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/micromamba/envs/hierarchical_reasoning_kg/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mtype_pprinters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_printers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                 deferred_pprinters=self.deferred_printers)\n\u001b[0;32m--> 702\u001b[0;31m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/micromamba/envs/hierarchical_reasoning_kg/lib/python3.7/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0;31m# printer registered in self.type_pprinters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;31m# deferred printer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/micromamba/envs/hierarchical_reasoning_kg/lib/python3.7/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    553\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreakable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;31m# Special case for 1-item tuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/micromamba/envs/hierarchical_reasoning_kg/lib/python3.7/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0;31m# printer registered in self.type_pprinters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;31m# deferred printer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/micromamba/envs/hierarchical_reasoning_kg/lib/python3.7/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36m_ordereddict_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_deque_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/micromamba/envs/hierarchical_reasoning_kg/lib/python3.7/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0;31m# printer registered in self.type_pprinters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;31m# deferred printer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/micromamba/envs/hierarchical_reasoning_kg/lib/python3.7/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    553\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreakable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;31m# Special case for 1-item tuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/micromamba/envs/hierarchical_reasoning_kg/lib/python3.7/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0;31m# printer registered in self.type_pprinters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;31m# deferred printer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/micromamba/envs/hierarchical_reasoning_kg/lib/python3.7/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    553\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreakable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;31m# Special case for 1-item tuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/micromamba/envs/hierarchical_reasoning_kg/lib/python3.7/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    392\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                                 \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__repr__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                             \u001b[0;32mreturn\u001b[0m \u001b[0m_repr_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_default_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/micromamba/envs/hierarchical_reasoning_kg/lib/python3.7/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;34m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;31m# Find newlines and replace them with p.break_()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/micromamba/envs/hierarchical_reasoning_kg/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__repr__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# All strings are unicode in Python 3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/micromamba/envs/hierarchical_reasoning_kg/lib/python3.7/site-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_str\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_str_intern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/micromamba/envs/hierarchical_reasoning_kg/lib/python3.7/site-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_str_intern\u001b[0;34m(inp)\u001b[0m\n\u001b[1;32m    356\u001b[0m                     \u001b[0mtensor_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m                     \u001b[0mtensor_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrided\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/micromamba/envs/hierarchical_reasoning_kg/lib/python3.7/site-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_tensor_str_with_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_formatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimag_formatter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_summarized_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msummarize\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_tensor_str_with_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/micromamba/envs/hierarchical_reasoning_kg/lib/python3.7/site-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36mget_summarized_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    272\u001b[0m         end = ([self[i]\n\u001b[1;32m    273\u001b[0m                for i in range(len(self) - PRINT_OPTS.edgeitems, len(self))])\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_summarized_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_summarized_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/micromamba/envs/hierarchical_reasoning_kg/lib/python3.7/site-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    272\u001b[0m         end = ([self[i]\n\u001b[1;32m    273\u001b[0m                for i in range(len(self) - PRINT_OPTS.edgeitems, len(self))])\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_summarized_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_summarized_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/micromamba/envs/hierarchical_reasoning_kg/lib/python3.7/site-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36mget_summarized_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mPRINT_OPTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medgeitems\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mPRINT_OPTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medgeitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mPRINT_OPTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medgeitems\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device"
     ]
    }
   ],
   "source": [
    "# CommonSense QA\n",
    "# RoBERTa-large + QA-GNN\n",
    "model_path: str = \"csqa_model_hf3.4.0.pt\"\n",
    "model = torch.load(model_path)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072ca60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch import nn\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_scatter import scatter\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfc408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one_hot(labels, C):\n",
    "    labels = labels.unsqueeze(1)\n",
    "    one_hot = torch.FloatTensor(labels.size(0), C).zero_().to(labels.device)\n",
    "    target = one_hot.scatter_(1, labels.data, 1)\n",
    "    target = Variable(target)\n",
    "    return target\n",
    "def freeze_net(module):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c9cdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    '''\n",
    "    This class is the encoder of text, represent as f_enc in paper\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, model_name, path = None):\n",
    "        super().__init__()\n",
    "        if path is None:\n",
    "            # download model from website\n",
    "            self.model = AutoModel.from_pretrained(model_name,output_hidden_states=True)\n",
    "        else:\n",
    "            self.model =AutoModel.from_pretrained(path)\n",
    "        self.output_size = self.model.config.hidden_size\n",
    "    def forward(self, *inputs, layers_id = -1):\n",
    "        '''\n",
    "        only support for transformer-based model\n",
    "        '''\n",
    "        input_ids, att_mask, token_types_ids, output_mask = inputs\n",
    "#         print(input_ids)\n",
    "#         print(att_mask)\n",
    "#         print(token_types_ids)\n",
    "        output = self.model(input_ids, attention_mask = att_mask, token_type_ids = token_types_ids)\n",
    "        all_hidden = output[-1]\n",
    "        final_hidden = all_hidden[layers_id]\n",
    "        out_vec = self.model.pooler(final_hidden)\n",
    "        return out_vec, all_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db8650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Customized_Embd(nn.Module):\n",
    "    '''\n",
    "    This class return the embedding of each concepts\n",
    "    '''\n",
    "    def __init__(self, num_concept, concept_in_dim, concept_out_dim, \n",
    "                 pretrained_concept_emb=None,use_contextualized = False, scale = 1.0, freeze_net_ = True,\n",
    "                init_range = 0.02):\n",
    "        super().__init__()\n",
    "        self.use_contextualized = use_contextualized\n",
    "        self.scale = scale\n",
    "        if not use_contextualized:\n",
    "            # get embedding \n",
    "            self.embd = nn.Embedding(num_concept, concept_in_dim)\n",
    "            if pretrained_concept_emb is not None:\n",
    "                print(pretrained_concept_emb.shape)\n",
    "                self.embd.weight.data.copy_(pretrained_concept_emb)\n",
    "            else:\n",
    "                self.embd.weight.data.normal_(mean = 0.0, std = init_range)\n",
    "            if freeze_net_:\n",
    "                freeze_net(self.embd)\n",
    "        \n",
    "        if concept_in_dim != concept_out_dim:\n",
    "            print(\"create projection\")\n",
    "            self.ln1 = nn.Linear(concept_in_dim, concept_out_dim)\n",
    "            # use gelu activation function\n",
    "            self.act = nn.GELU()\n",
    "            \n",
    "    def forward(self, index, contextualized_emb=None):\n",
    "        '''\n",
    "        index shape : (batch_size, a)\n",
    "        contextualized_embd shape : (batch_size, b, embd_size)\n",
    "        '''\n",
    "        if contextualized_emb is not None:\n",
    "            assert index.shape[0] == contextualized_emb.shape[0]\n",
    "            if hasattr(self, 'ln1'):\n",
    "                contextualized_emb = self.act(self.ln1(contextualized_emb * self.scale))\n",
    "#                 print(\"doing projection\")\n",
    "#                 print(contextualized_emb.shape)\n",
    "            else:\n",
    "                contextualized_emb *= self.scale\n",
    "            embd_dim = contextualized_emb.shape[-1]\n",
    "            #print(embd_dim)\n",
    "            return contextualized_emb.gather(1, index.unsqueeze(-1).expand(-1,-1, embd_dim))\n",
    "        else:\n",
    "            if hasattr(self, 'ln1'):\n",
    "                return self.act(self.ln1(self.embd(index) * self.scale))\n",
    "            else:\n",
    "                return self.embd(index) * self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e015d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAGNN_message_passing(nn.Module):\n",
    "    def __init__(self, args, k, node_type, edge_type,input_size, hidden_size, output_size,\n",
    "                dropout_rate =0.1):\n",
    "        '''\n",
    "        params:\n",
    "            1. args, extract args\n",
    "            2. k, num_layer\n",
    "            3. node_type, num_node_type\n",
    "            4. edge_type, num_edge_type\n",
    "            5. input_size, initial embedding\n",
    "            6. hidden_size, hidden embedding\n",
    "            7. output_size, output_embedding\n",
    "            8. dropout rate, rate of dropout\n",
    "        '''\n",
    "        super().__init__()\n",
    "        # since need to use sequential\n",
    "        assert input_size == output_size\n",
    "        self.args = args\n",
    "        self.node_type = node_type\n",
    "        self.edge_type = edge_type\n",
    "        assert input_size == hidden_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embd_node_type = nn.Linear(self.node_type, hidden_size//2)\n",
    "        # define basis function\n",
    "#         print(\"input_size\", input_size)\n",
    "#         print(\"hidden_size\", hidden_size)\n",
    "#         print(\"output_size\", output_size)\n",
    "        self.basis_f = 'sin'\n",
    "        self.embd_score = nn.Linear(hidden_size//2, hidden_size//2)\n",
    "        # create edge_encoder\n",
    "        # in paper, it's represent edge info\n",
    "        self.edge_encoder = nn.Sequential(nn.Linear(edge_type +1 + node_type *2, hidden_size),nn.BatchNorm1d(hidden_size), nn.ReLU(), torch.nn.Linear(hidden_size, hidden_size))\n",
    "        self.k = k\n",
    "        # k layer per each\n",
    "        self.gnn_layers = nn.ModuleList([GATConvE(args=args, embd_dim= hidden_size,num_node_type= node_type, num_edges= edge_type, edge_encoder= self.edge_encoder) for _ in range(k)])\n",
    "        self.vh = nn.Linear(input_size, output_size)\n",
    "        self.vx = nn.Linear(hidden_size, output_size)\n",
    "        self.act = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.dropout_rate = dropout_rate\n",
    "    def message_passing_helpler(self, _X, edge_index, edge_type, _node_type, _node_feature_extra):\n",
    "        for i in range(self.k):\n",
    "            _X = self.gnn_layers[i](_X, edge_index, edge_type, _node_type, _node_feature_extra)\n",
    "            _X = self.act(_X)\n",
    "            _X = torch.nn.functional.dropout(_X, self.dropout_rate, training = self.training)\n",
    "        return _X\n",
    "    \n",
    "    def forward(self, H,A, node_type, node_score, cache_output =False):\n",
    "        '''\n",
    "        H : node features from previous layer, shape like (batch_size, n_node,dim)\n",
    "        A : (edge_index, edge_type), tuple\n",
    "        node_type: long tensor of shape (batch_size, n_node):\n",
    "            in this form, which is:\n",
    "                0 == question, 1 == answer, 2 == others, 3 == context\n",
    "        node_score, tensor of shape (batch_size, num_node ,1)\n",
    "        '''\n",
    "        # embed type\n",
    "        bs, num_node = node_type.size()\n",
    "        T = make_one_hot(node_type.view(-1).contiguous(),self.node_type).view(bs, num_node, self.node_type)\n",
    "        # get node type embedding, linearly transform\n",
    "        node_type_embd = self.act(self.embd_node_type(T)) # shape [bs, num_node, dim/2]\n",
    "        \n",
    "        # embedding score\n",
    "        # shape is [1,1, dim/2]\n",
    "        # create a sin form weight\n",
    "        js = torch.arange(self.hidden_size//2).unsqueeze(0).unsqueeze(0).float().to(node_type.device)\n",
    "        js = torch.pow(1.1, js)\n",
    "        # [bs, num_node, dim/2]\n",
    "        B = torch.sin(js * node_score)\n",
    "        # passing linearly transform layer with GELU activation\n",
    "        node_score_embd = self.act(self.embd_score(B))\n",
    "#         print(\"node_score_embd shape\",node_score_embd.shape)\n",
    "#         print(\"node_type_embd shape\", node_type_embd.shape)\n",
    "        # get adjaency matrix and it's relation \n",
    "        edge_index, edge_type = A\n",
    "        tmp = H.view(-1, H.size(2)).contiguous()# [bs * num_node, dim]\n",
    "        _node_type = node_type.view(-1).contiguous() #[bs*num_node, ]\n",
    "        \n",
    "        # create node_feature_extra, which concat node_type_embedding and node_score_embd\n",
    "        # shape [bs*num_node, dim]\n",
    "        node_feature_extra = torch.cat([node_type_embd, node_score_embd],dim = 2).view(_node_type.size(0), -1).contiguous()\n",
    "#         print(\"feature_extra shape\", node_feature_extra.shape)\n",
    "        X = self.message_passing_helpler(tmp, edge_index, edge_type, _node_type, node_feature_extra)\n",
    "        X = X.view(node_type.size(0),node_type.size(1), -1)# [bs, num_node, dim]\n",
    "        output = self.dropout(self.act(self.vh(H) + self.vx(X)))\n",
    "        return output\n",
    "# implement GATConvE\n",
    "# by using message passing\n",
    "\n",
    "class GATConvE(MessagePassing):\n",
    "    '''\n",
    "    args:\n",
    "        1. embd_dim, the dimension of GNN hidden states\n",
    "        2. num_node_type: number of node types, which is 4\n",
    "        3. num_edges, number of edges\n",
    "    '''\n",
    "    def __init__(self, args, embd_dim, num_node_type, num_edges, edge_encoder,num_heads = 4, aggr = \"add\"):\n",
    "        super().__init__(aggr=aggr)\n",
    "        self.args = args\n",
    "        assert embd_dim %2 == 0\n",
    "        self.embd_dim = embd_dim\n",
    "        self.num_node_type = num_node_type\n",
    "        self.num_edges = num_edges\n",
    "        self.edge_encoder = edge_encoder\n",
    "        \n",
    "        # calculate attention part\n",
    "        self.num_heads = num_heads\n",
    "        assert embd_dim % num_heads == 0\n",
    "        # it has repeated in early part\n",
    "        self.dim_per_head = embd_dim // num_heads\n",
    "        # define linear transformation for key, value, query\n",
    "        # query: q = (hidden, node_feature, score_feature)\n",
    "        # key: k = (hidden, node_features, relation_features, score)\n",
    "        self.ln_key = nn.Linear(3 * embd_dim, num_heads * self.dim_per_head)\n",
    "        self.ln_msg = nn.Linear(3 * embd_dim, num_heads * self.dim_per_head)\n",
    "        self.ln_query = nn.Linear(2 * embd_dim, num_heads * self.dim_per_head)\n",
    "        \n",
    "        self._alpha =None\n",
    "        self.mlp = nn.Sequential(nn.Linear(embd_dim, embd_dim),nn.BatchNorm1d(embd_dim), nn.ReLU(),\n",
    "                                nn.Linear(embd_dim, embd_dim))\n",
    "    def forward(self, x, edge_index, edge_type, node_type, node_feature_extra):\n",
    "        '''\n",
    "        x: dimension of nodes [num_nodes, embd_dim]\n",
    "        edge_index: [2,num_Edges]\n",
    "        edge_type: [1,num_edges]\n",
    "        node_type: [1,num_nodes]\n",
    "        node_feature_extra [N, dim]\n",
    "        '''\n",
    "#         print(\"x \", x.shape)\n",
    "        # here use the one-hot vector to represent each edge\n",
    "        # unlike compgcn use basis vector to represent, here use one-hot\n",
    "        edge_vec = make_one_hot(edge_type, self.num_edges + 1)# [num_edges, 39]\n",
    "        # create a shape likes [num_node, num_edges]\n",
    "        self_edge_vec = torch.zeros(x.shape[0], self.num_edges + 1).to(edge_vec.device)\n",
    "        # for last edge vector\n",
    "        self_edge_vec[:, self.num_edges] = 1\n",
    "        # get head, tail embedding\n",
    "        \n",
    "        #------------------------------ what is node_type here? source or target ? or dataset own node type?\n",
    "        # ----------------------------- need to be checked\n",
    "        head_type = node_type[edge_index[0]]# head is source\n",
    "        tail_type = node_type[edge_index[1]]# tail is target\n",
    "        head_vec = make_one_hot(head_type, self.num_node_type) #[E, 4]\n",
    "        tail_vec = make_one_hot(tail_type, self.num_node_type)# [E, 4]\n",
    "        headtail_vec = torch.cat([head_vec, tail_vec], dim=1)# [E, 8]\n",
    "        self_head_vec = make_one_hot(node_type, self.num_node_type)#[N, 4]\n",
    "        self_head_tail_vec = torch.cat([self_head_vec, self_head_vec], dim = 1)# [N,8]\n",
    "        \n",
    "        edge_vec = torch.cat([edge_vec, self_edge_vec], dim = 0)# [E+N, ?]\n",
    "        headtail_vec = torch.cat([headtail_vec, self_head_tail_vec], dim = 0)# [E+N,?]\n",
    "        edge_embeddings = self.edge_encoder(torch.cat([edge_vec, headtail_vec], dim=1)) #[E+N, emb_dim]\n",
    "        \n",
    "        # add self loop\n",
    "        loop_idx = torch.arange(0, x.shape[0], dtype= torch.long, device=edge_index.device)\n",
    "        loop_idx = loop_idx.unsqueeze(0).repeat(2,1)\n",
    "#         print(\"loop index\", loop_idx.shape)\n",
    "#         print(\"edge_embedding\", edge_embeddings.shape)\n",
    "        edge_index = torch.cat([edge_index, loop_idx], dim = 1)\n",
    "        x = torch.cat([x, node_feature_extra], dim = 1)\n",
    "#         print(\"x\", x.shape)\n",
    "        x = (x,x)\n",
    "        aggr_out = self.propagate(edge_index, x = x, edge_attr = edge_embeddings)# [N, embd_dim]\n",
    "        output = self.mlp(aggr_out)\n",
    "        return output\n",
    "    \n",
    "    def message(self, edge_index, x_i, x_j, edge_attr):\n",
    "        '''\n",
    "        This message function process the message\n",
    "        '''\n",
    "        # print (\"edge_attr.size()\", edge_attr.size()) #[E, emb_dim]\n",
    "        # print (\"x_j.size()\", x_j.size()) #[E, emb_dim]\n",
    "        # print (\"x_i.size()\", x_i.size()) #[E, emb_dim]\n",
    "        assert len(edge_attr.size()) == 2\n",
    "        assert edge_attr.size(1) == self.embd_dim\n",
    "        assert x_i.size(1) == x_j.size(1) == 2*self.embd_dim\n",
    "        assert x_i.size(0) == x_j.size(0) == edge_attr.size(0) == edge_index.size(1)\n",
    "        # for key, query, value\n",
    "        # (E, heads, dim)\n",
    "#         print(x_i.shape, edge_attr.shape)\n",
    "        key = self.ln_key(torch.cat([x_i,edge_attr], dim = 1)).view(-1, self.num_heads, self.dim_per_head)\n",
    "        msg = self.ln_msg(torch.cat([x_j, edge_attr], dim = 1)).view(-1, self.num_heads, self.dim_per_head)\n",
    "        query = self.ln_query(x_j).view(-1, self.num_heads, self.dim_per_head)\n",
    "        \n",
    "        # calculate softmax\n",
    "        query = query / math.sqrt(self.dim_per_head)\n",
    "        scores = (query * key).sum(2)# shape is [E, num_heads]\n",
    "        # select source index\n",
    "        src_node_index = edge_index[0]\n",
    "        # group by source node index, and calculate score\n",
    "        alpha = softmax(scores, src_node_index)\n",
    "       # print(\"attention score\", alpha.shape)\n",
    "        num_edges = edge_index.shape[1]\n",
    "        num_nodes = int(src_node_index.max()) + 1\n",
    "       # print(\"num edges\", src_node_index.shape)\n",
    "        #print(\"num edges\", num_edges)\n",
    "        # 我不知道为什么这里根据 out degree调整了 attention score\n",
    "        # adjust by outgoing degree of src\n",
    "        # this operation get the out degree from source node\n",
    "        ones = torch.full((num_edges,), 1.0, dtype=torch.float).to(edge_index.device)\n",
    "        # shape (E,)\n",
    "        src_node_Edge_count = scatter(ones, src_node_index, dim=0, dim_size= num_nodes,\n",
    "                                     reduce=\"sum\")[src_node_index]\n",
    "       #print(\"out degree shape\", src_node_Edge_count.shape)\n",
    "        alpha = alpha * src_node_Edge_count.unsqueeze(1)# [E, num_heads]\n",
    "        \n",
    "        out = msg * alpha.view(-1, self.num_heads, 1)\n",
    "        return out.view(-1, self.num_heads * self.dim_per_head)#[E, embd_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf52ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixVectorScaledDotProductAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        q: tensor of shape (n*b, d_k)\n",
    "        k: tensor of shape (n*b, l, d_k)\n",
    "        v: tensor of shape (n*b, l, d_v)\n",
    "        returns: tensor of shape (n*b, d_v), tensor of shape(n*b, l)\n",
    "        \"\"\"\n",
    "        attn = (q.unsqueeze(1) * k).sum(2)  # (n*b, l)\n",
    "        attn = attn / self.temperature\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask, -np.inf)\n",
    "        attn = self.softmax(attn)\n",
    "        attn = self.dropout(attn)\n",
    "        output = (attn.unsqueeze(2) * v).sum(1)\n",
    "        return output, attn\n",
    "class MultiheadAttPoolLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, n_head, d_q_original, d_k_original, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_k_original % n_head == 0  # make sure the outpute dimension equals to d_k_origin\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k_original // n_head\n",
    "        self.d_v = d_k_original // n_head\n",
    "\n",
    "        self.w_qs = nn.Linear(d_q_original, n_head * self.d_k)\n",
    "        self.w_ks = nn.Linear(d_k_original, n_head * self.d_k)\n",
    "        self.w_vs = nn.Linear(d_k_original, n_head * self.d_v)\n",
    "\n",
    "        nn.init.normal_(self.w_qs.weight, mean=0, std=np.sqrt(2.0 / (d_q_original + self.d_k)))\n",
    "        nn.init.normal_(self.w_ks.weight, mean=0, std=np.sqrt(2.0 / (d_k_original + self.d_k)))\n",
    "        nn.init.normal_(self.w_vs.weight, mean=0, std=np.sqrt(2.0 / (d_k_original + self.d_v)))\n",
    "\n",
    "        self.attention = MatrixVectorScaledDotProductAttention(temperature=np.power(self.d_k, 0.5))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, mask=None):\n",
    "        \"\"\"\n",
    "        q: tensor of shape (b, d_q_original)\n",
    "        k: tensor of shape (b, l, d_k_original)\n",
    "        mask: tensor of shape (b, l) (optional, default None)\n",
    "        returns: tensor of shape (b, n*d_v)\n",
    "        \"\"\"\n",
    "        n_head, d_k, d_v = self.n_head, self.d_k, self.d_v\n",
    "\n",
    "        bs, _ = q.size()\n",
    "        bs, len_k, _ = k.size()\n",
    "\n",
    "        qs = self.w_qs(q).view(bs, n_head, d_k)  # (b, n, dk)\n",
    "        ks = self.w_ks(k).view(bs, len_k, n_head, d_k)  # (b, l, n, dk)\n",
    "        vs = self.w_vs(k).view(bs, len_k, n_head, d_v)  # (b, l, n, dv)\n",
    "\n",
    "        qs = qs.permute(1, 0, 2).contiguous().view(n_head * bs, d_k)\n",
    "        ks = ks.permute(2, 0, 1, 3).contiguous().view(n_head * bs, len_k, d_k)\n",
    "        vs = vs.permute(2, 0, 1, 3).contiguous().view(n_head * bs, len_k, d_v)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.repeat(n_head, 1)\n",
    "        output, attn = self.attention(qs, ks, vs, mask=mask)\n",
    "\n",
    "        output = output.view(n_head, bs, d_v)\n",
    "        output = output.permute(1, 0, 2).contiguous().view(bs, n_head * d_v)  # (b, n*dv)\n",
    "        output = self.dropout(output)\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c62297",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-layer perceptron\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_layers: number of hidden layers\n",
    "    \"\"\"\n",
    "    activation_classes = {'gelu': nn.GELU, 'relu': nn.ReLU, 'tanh': nn.Tanh}\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout, batch_norm=False,\n",
    "                 init_last_layer_bias_to_zero=False, layer_norm=False, activation='gelu'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.batch_norm = batch_norm\n",
    "        self.layer_norm = layer_norm\n",
    "\n",
    "        assert not (self.batch_norm and self.layer_norm)\n",
    "\n",
    "        self.layers = nn.Sequential()\n",
    "        for i in range(self.num_layers + 1):\n",
    "            n_in = self.input_size if i == 0 else self.hidden_size\n",
    "            n_out = self.hidden_size if i < self.num_layers else self.output_size\n",
    "            self.layers.add_module(f'{i}-Linear', nn.Linear(n_in, n_out))\n",
    "            if i < self.num_layers:\n",
    "                self.layers.add_module(f'{i}-Dropout', nn.Dropout(self.dropout))\n",
    "                if self.batch_norm:\n",
    "                    self.layers.add_module(f'{i}-BatchNorm1d', nn.BatchNorm1d(self.hidden_size))\n",
    "                if self.layer_norm:\n",
    "                    self.layers.add_module(f'{i}-LayerNorm', nn.LayerNorm(self.hidden_size))\n",
    "                self.layers.add_module(f'{i}-{activation}', self.activation_classes[activation.lower()]())\n",
    "        if init_last_layer_bias_to_zero:\n",
    "            self.layers[-1].bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        return self.layers(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f6cd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAGNN(nn.Module):\n",
    "    '''\n",
    "    Layer of GNN\n",
    "    '''\n",
    "    def __init__(self, args, k, n_node_type, n_edge_type, sent_dim, n_concept, concept_dim, concept_in_dim, \n",
    "                n_attention_head, fc_dim, num_fc_layer, drop_embd, drop_gnn,drop_fc,\n",
    "                 pretrained_concept_emb=None,freeze_ent_emb=True,init_range=0.02):\n",
    "        '''\n",
    "        params:\n",
    "            1. args, extra args\n",
    "            2. k, num_layers\n",
    "            3. n_node_type, num of node type\n",
    "            4. n_edge_type, num of edge type\n",
    "            5. sent_dim, sentence dimension\n",
    "            6. n_concept, num of concept\n",
    "            7. concept_dim, concept out dimension\n",
    "            8. concepte_in_dim, concept in dimension\n",
    "            9. n_attention_head, num of attention head for multi-head-attention\n",
    "            10. fc_dim, linear transform dimension\n",
    "            11. num_fc_layer, linear transform layer number\n",
    "            12. drop_embd, dropout rate of embd\n",
    "            13. drop_gnn, dropout rate of gnn\n",
    "            14. drop_fc, dropout rate of fc\n",
    "        '''\n",
    "        super().__init__()\n",
    "        print(\"freeze_ent_emb\", freeze_ent_emb)\n",
    "        self.init_range = init_range\n",
    "#         print(\"concept_in_dim\", concept_in_dim)\n",
    "#         print(\"concept_out_dim\", concept_dim)\n",
    "        self.concept_embd = Customized_Embd(num_concept=n_concept, concept_in_dim= concept_in_dim, concept_out_dim = concept_dim,\n",
    "                                           use_contextualized=False, pretrained_concept_emb = pretrained_concept_emb,\n",
    "                                            freeze_net_=freeze_ent_emb)\n",
    "        # project sentence dimension to concept dimension\n",
    "        self.ln1 = nn.Linear(sent_dim, concept_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.concept_dim = concept_dim\n",
    "        \n",
    "        #define gnn layer\n",
    "        self.gnn = QAGNN_message_passing(args, k = k, node_type= n_node_type, edge_type= n_edge_type,\n",
    "                                        input_size=concept_dim, hidden_size= concept_dim, output_size= concept_dim, \n",
    "                                         dropout_rate= drop_gnn)\n",
    "        # pooler for sentence \n",
    "        self.pooler = MultiheadAttPoolLayer(n_attention_head, sent_dim, concept_dim)\n",
    "        # MLP\n",
    "        self.mlp = MLP(concept_dim + sent_dim + concept_dim, fc_dim, 1, num_fc_layer, drop_fc,\n",
    "                      layer_norm= True)\n",
    "        self.dropout_ebd = nn.Dropout(drop_embd)\n",
    "        self.dropout_fc = nn.Dropout(drop_fc)\n",
    "        if init_range > 0 :\n",
    "            self.apply(self.init_weights)\n",
    "    \n",
    "    def init_weights(self,module):\n",
    "        if isinstance(module, (nn.Embedding, nn.Linear)):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.init_range)\n",
    "            if hasattr(module, 'bias') and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    \n",
    "    def forward(self, sent_vecs, concept_ids, node_type_ids, node_scores, adj_length,\n",
    "               adj, embd_data = None, cache_output = False):\n",
    "        '''\n",
    "        params:\n",
    "            1. sent_vecs: sentence embedding (batch_size, dim_sent)\n",
    "            2. concepts_ids: (batch_size, num_nodes)\n",
    "            3. adj: (edge_index, edge_type)\n",
    "            4. adj_length:(batch_size ,)\n",
    "            5. node_type_ids: (batch_size, num_nodes)\n",
    "            6. node_socres: (batch_size, num_nodes, 1)\n",
    "        return:\n",
    "            (batch_size, 1)\n",
    "        '''\n",
    "        # project dimnsion of sentence to node_dim\n",
    "        # shape (batch_size, 1, node_dim)\n",
    "        # which is contextualize part\n",
    "        sent2node_vecs = self.act(self.ln1(sent_vecs)).unsqueeze(1)\n",
    "        # get concepts ids, remove the context part, get the embedding \n",
    "        # (bs, num_node-1, dim_node)\n",
    "        gnn_input1 = self.concept_embd(concept_ids[:,1:]-1, embd_data).to(node_type_ids.device)\n",
    "        # concat the context embd z and others embd\n",
    "        gnn_input = self.dropout_ebd(torch.cat([sent2node_vecs, gnn_input1], dim = 1))   \n",
    "        \n",
    "        \n",
    "        #------------------- normalize the node score ---------------------------\n",
    "        # create mask\n",
    "        # 0 means masked out [bs, num_node]\n",
    "        mask_ = (torch.arange(node_scores.size(1), device = node_scores.device) < adj_length.unsqueeze(1)).float()\n",
    "        node_scores = -node_scores\n",
    "        # 避免梯度爆炸, 此前已排序\n",
    "        node_scores = node_scores - node_scores[:,0:1,:]\n",
    "        # [bs, num_node]\n",
    "        node_scores = node_scores.squeeze(2)\n",
    "        # get the mask scores, means only calculate scores for each batch\n",
    "        node_scores *= mask_\n",
    "        mean_norm = (torch.abs(node_scores)).sum(dim = 1)/ adj_length\n",
    "        # [bs, n_node]\n",
    "        node_scores = node_scores / (mean_norm.unsqueeze(1) + 1e-05)\n",
    "        node_scores = node_scores.unsqueeze(2)#[bs, n_node, 1]\n",
    "        \n",
    "        gnn_output = self.gnn(gnn_input, adj, node_type_ids, node_scores)\n",
    "        # extract updated Z vectors in paper\n",
    "        z_vecs = gnn_output[:,0]\n",
    "        mask = torch.arange(node_type_ids.size(1), device=node_type_ids.device) >= adj_length.unsqueeze(1) #1 means masked out\n",
    "        # mask the non-this-batch node and context node\n",
    "        mask = mask | (node_type_ids == 3)\n",
    "        mask[mask.all(1),0] = 0\n",
    "        sent_vecs_for_pooler = sent_vecs\n",
    "        # finall use sentence as query, gnn_output as key-value pair with mask\n",
    "        graph_vecs, pool_attn = self.pooler(sent_vecs_for_pooler, gnn_output, mask)\n",
    "        \n",
    "        # finally concat vector \n",
    "        concat = self.dropout_fc(torch.cat((graph_vecs, sent_vecs, z_vecs), dim = 1))\n",
    "        # pass mlp to update\n",
    "        logits = self.mlp(concat)\n",
    "        return logits, pool_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316d107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM_QAGNN(nn.Module):\n",
    "    def __init__(self, args, model_name, k, num_node_type, num_edge_type, \n",
    "                num_concept, concept_dim, concept_in_dim, n_attention_head, fc_dim,\n",
    "                num_fc_layer, drop_embd, drop_gnn, drop_fc, pretrained_concept_emb=None,freeze_ent_emb = True,\n",
    "                init_range = 0.0):\n",
    "        super().__init__()\n",
    "        self.encoder = TextEncoder(model_name)\n",
    "        self.decoder = QAGNN(args, k, num_node_type, num_edge_type, self.encoder.output_size,\n",
    "                            num_concept,concept_dim, concept_in_dim, n_attention_head, fc_dim,\n",
    "                            num_fc_layer, drop_embd, drop_gnn, drop_fc, pretrained_concept_emb,\n",
    "                             freeze_ent_emb, init_range)\n",
    "        \n",
    "    def forward(self, *inputs, layer_id=-1, cache_output=False, detail=False):\n",
    "        \"\"\"\n",
    "        sent_vecs: (batch_size, num_choice, d_sent)    -> (batch_size * num_choice, d_sent)\n",
    "        concept_ids: (batch_size, num_choice, n_node)  -> (batch_size * num_choice, n_node)\n",
    "        node_type_ids: (batch_size, num_choice, n_node) -> (batch_size * num_choice, n_node)\n",
    "        adj_lengths: (batch_size, num_choice)          -> (batch_size * num_choice, )\n",
    "        adj -> edge_index, edge_type\n",
    "            edge_index: list of (batch_size, num_choice) -> list of (batch_size * num_choice, ); each entry is torch.tensor(2, E(variable))\n",
    "                                                         -> (2, total E)\n",
    "            edge_type:  list of (batch_size, num_choice) -> list of (batch_size * num_choice, ); each entry is torch.tensor(E(variable), )\n",
    "                                                         -> (total E, )\n",
    "        returns: (batch_size, 1)\n",
    "        \"\"\"\n",
    "        bs, nc = inputs[0].size(0), inputs[0].size(1)\n",
    "\n",
    "        #Here, merge the batch dimension and the num_choice dimension\n",
    "        edge_index_orig, edge_type_orig = inputs[-2:]\n",
    "        _inputs = [x.view(x.size(0) * x.size(1), *x.size()[2:]) for x in inputs[:-6]] + [x.view(x.size(0) * x.size(1), *x.size()[2:]) for x in inputs[-6:-2]] + [sum(x,[]) for x in inputs[-2:]]\n",
    "\n",
    "        *lm_inputs, concept_ids, node_type_ids, node_scores, adj_lengths, edge_index, edge_type = _inputs\n",
    "#         print(lm_inputs[0].shape)\n",
    "#         print(lm_inputs[1].shape)\n",
    "#         print(lm_inputs[2].shape)\n",
    "#         print(lm_inputs[3].shape)\n",
    "#         print(\"concept ids shape \", concept_ids.shape)\n",
    "#         print(\"node_type_ids \", node_type_ids.shape)\n",
    "#         print(\"adj_lenght \", adj_lengths.shape)\n",
    "        edge_index, edge_type = self.batch_graph(edge_index, edge_type, concept_ids.size(1))\n",
    "        adj = (edge_index.to(node_type_ids.device), edge_type.to(node_type_ids.device)) #edge_index: [2, total_E]   edge_type: [total_E, ]\n",
    "\n",
    "        sent_vecs, all_hidden_states = self.encoder(*lm_inputs, layers_id=layer_id)\n",
    "        logits, attn = self.decoder(sent_vecs.to(node_type_ids.device),\n",
    "                                    concept_ids,\n",
    "                                    node_type_ids, node_scores, adj_lengths, adj,\n",
    "                                    embd_data=None)\n",
    "        logits = logits.view(bs, nc)\n",
    "        \n",
    "        return logits, attn\n",
    "    def batch_graph(self, edge_index_init, edge_type_init, n_nodes):\n",
    "        #edge_index_init: list of (n_examples, ). each entry is torch.tensor(2, E)\n",
    "        #edge_type_init:  list of (n_examples, ). each entry is torch.tensor(E, )\n",
    "        n_examples = len(edge_index_init)\n",
    "        edge_index = [edge_index_init[_i_] + _i_ * n_nodes for _i_ in range(n_examples)]\n",
    "        edge_index = torch.cat(edge_index, dim=1) #[2, total_E]\n",
    "        edge_type = torch.cat(edge_type_init, dim=0) #[total_E, ]\n",
    "        return edge_index, edge_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6edb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    '''\n",
    "    EXAMPLE class\n",
    "    '''\n",
    "    def __init__(self, example_id, question, contexts, endings, label=None):\n",
    "        self.example_id = example_id\n",
    "        self.question = question\n",
    "        self.contexts = contexts\n",
    "        self.endings = endings\n",
    "        self.label = label\n",
    "\n",
    "class InputFeatures(object):\n",
    "    def __init__(self, example_id, choices_features, label):\n",
    "            self.example_id = example_id\n",
    "            self.choices_features = [\n",
    "                {\n",
    "                    'input_ids': input_ids,\n",
    "                    'input_mask': input_mask,\n",
    "                    'segment_ids': segment_ids,\n",
    "                    'output_mask': output_mask,\n",
    "                }\n",
    "                for _, input_ids, input_mask, segment_ids, output_mask in choices_features\n",
    "            ]\n",
    "            self.label = label\n",
    "\n",
    "def read_examples(input_files):\n",
    "    with open(input_files, \"r\", encoding=\"utf-8\") as f:\n",
    "        examples = []\n",
    "        for line in f.readlines():\n",
    "            json_dic = json.loads(line)\n",
    "            #print(json_dic)\n",
    "            label = ord(json_dic[\"answerKey\"]) - ord(\"A\") if 'answerKey' in json_dic else 0\n",
    "            contexts = json_dic[\"question\"][\"stem\"]\n",
    "            if \"para\" in json_dic:\n",
    "                contexts = json_dic[\"para\"] + \" \" + contexts\n",
    "            if \"fact1\" in json_dic:\n",
    "                contexts = json_dic[\"fact1\"] + \" \" + contexts\n",
    "            examples.append(\n",
    "            InputExample(\n",
    "                    example_id=json_dic[\"id\"],\n",
    "                    contexts=[contexts] * len(json_dic[\"question\"][\"choices\"]),\n",
    "                    question=\"\",\n",
    "                    endings=[ending[\"text\"] for ending in json_dic[\"question\"][\"choices\"]],\n",
    "                    label=label\n",
    "                ))\n",
    "        return examples\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "        \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "        # This is a simple heuristic which will always truncate the longer sequence\n",
    "        # one token at a time. This makes more sense than truncating an equal percent\n",
    "        # of tokens from each, since if one sequence is very short then each token\n",
    "        # that's truncated likely contains more information than a longer sequence.\n",
    "        while True:\n",
    "            total_length = len(tokens_a) + len(tokens_b)\n",
    "            if total_length <= max_length:\n",
    "                break\n",
    "            if len(tokens_a) > len(tokens_b):\n",
    "                tokens_a.pop()\n",
    "            else:\n",
    "                tokens_b.pop()\n",
    "    \n",
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer,\n",
    "                               cls_token='[CLS]',\n",
    "                                     cls_token_segment_id=0,\n",
    "                                     sep_token='[SEP]',\n",
    "                                     sequence_a_segment_id=0,\n",
    "                                     sequence_b_segment_id=1,\n",
    "                                     sep_token_extra=False,\n",
    "                                     pad_token_segment_id=0,\n",
    "                                     pad_token=0,\n",
    "                                     mask_padding_with_zero=True):\n",
    "        label_map = {label: i for i, label in enumerate(label_list)}\n",
    "        print(label_map)\n",
    "        features = []\n",
    "        for ex_index, example in enumerate(tqdm(examples)):\n",
    "            choices_features = []\n",
    "            for ending_idx, (context, ending) in enumerate(zip(example.contexts, example.endings)):\n",
    "                tokens_a = tokenizer.tokenize(context)\n",
    "                tokens_b = tokenizer.tokenize(example.question + \" \" + ending)\n",
    "\n",
    "                special_tokens_count = 4 if sep_token_extra else 3\n",
    "                _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - special_tokens_count)\n",
    "                tokens = tokens_a + [sep_token]\n",
    "                if sep_token_extra:\n",
    "                    # roberta uses an extra separator b/w pairs of sentences\n",
    "                    tokens += [sep_token]\n",
    "\n",
    "                segment_ids = [sequence_a_segment_id] * len(tokens)\n",
    "\n",
    "                if tokens_b:\n",
    "                    tokens += tokens_b + [sep_token]\n",
    "                    segment_ids += [sequence_b_segment_id] * (len(tokens_b) + 1)\n",
    "                tokens = [cls_token] + tokens\n",
    "                segment_ids = [cls_token_segment_id] + segment_ids\n",
    "                # take input ids\n",
    "                input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "                input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "                special_token_id = tokenizer.convert_tokens_to_ids([cls_token, sep_token])\n",
    "                output_mask = [1 if id in special_token_id else 0 for id in input_ids]  # 1 for mask\n",
    "                \n",
    "                # padding for 0 to sequence length\n",
    "                padding_length = max_seq_length - len(input_ids)\n",
    "                input_ids = input_ids + ([pad_token] * padding_length)\n",
    "                input_mask = input_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "                output_mask = output_mask + ([1] * padding_length)\n",
    "                segment_ids = segment_ids + ([pad_token_segment_id] * padding_length)\n",
    "                \n",
    "                assert len(input_ids) == max_seq_length\n",
    "                assert len(output_mask) == max_seq_length\n",
    "                assert len(input_mask) == max_seq_length\n",
    "                assert len(segment_ids) == max_seq_length\n",
    "                choices_features.append((tokens, input_ids, input_mask, segment_ids, output_mask))\n",
    "            label = label_map[example.label]\n",
    "            features.append(InputFeatures(example_id=example.example_id, choices_features=choices_features, label=label))\n",
    "\n",
    "        return features\n",
    "    \n",
    "def select_field(features, field):\n",
    "        return [[choice[field] for choice in feature.choices_features] for feature in features]\n",
    "\n",
    "def convert_features_to_tensors(features):\n",
    "    all_input_ids = torch.tensor(select_field(features, 'input_ids'), dtype=torch.long)\n",
    "    all_input_mask = torch.tensor(select_field(features, 'input_mask'), dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor(select_field(features, 'segment_ids'), dtype=torch.long)\n",
    "    all_output_mask = torch.tensor(select_field(features, 'output_mask'), dtype=torch.bool)\n",
    "    all_label = torch.tensor([f.label for f in features], dtype=torch.long)\n",
    "    print(all_input_ids.shape)\n",
    "    print(all_input_mask.shape)\n",
    "    print(all_segment_ids.shape)\n",
    "    print(all_output_mask.shape)\n",
    "    print(all_label.shape)\n",
    "    return all_input_ids, all_input_mask, all_segment_ids, all_output_mask, all_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b372c744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tensor(statement_jsonl_path, model_name = \"roberta-large\", max_seq_lenth = 128):\n",
    "    examples = read_examples(statement_jsonl_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    features = convert_examples_to_features(examples, list(range(len(examples[0].endings))),\n",
    "                                           max_seq_lenth, tokenizer=tokenizer,cls_token=tokenizer.cls_token,\n",
    "                                                sep_token=tokenizer.sep_token,\n",
    "                                                sep_token_extra=True,\n",
    "                                            sequence_b_segment_id=0,\n",
    "                                           )\n",
    "    # convert feature to tensor\n",
    "    example_ids = [f.example_id for f in features]\n",
    "    *data_tensors, all_label = convert_features_to_tensors(features)\n",
    "    return (example_ids, all_label, *data_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f42f3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sparse_adj_data_with_contextnode(adj_pk_path, max_node_num, num_choice, args):\n",
    "    cache_path = adj_pk_path +'.loaded_cache'\n",
    "    use_cache = True\n",
    "\n",
    "    if use_cache and not os.path.exists(cache_path):\n",
    "        use_cache = False\n",
    "\n",
    "    if use_cache:\n",
    "        with open(cache_path, 'rb') as f:\n",
    "            adj_lengths_ori, concept_ids, node_type_ids, node_scores, adj_lengths, edge_index, edge_type, half_n_rel = pickle.load(f)\n",
    "    ori_adj_mean  = adj_lengths_ori.float().mean().item()\n",
    "    ori_adj_sigma = np.sqrt(((adj_lengths_ori.float() - ori_adj_mean)**2).mean().item())\n",
    "    print('| ori_adj_len: mu {:.2f} sigma {:.2f} | adj_len: {:.2f} |'.format(ori_adj_mean, ori_adj_sigma, adj_lengths.float().mean().item()) +\n",
    "          ' prune_rate： {:.2f} |'.format((adj_lengths_ori > adj_lengths).float().mean().item()) +\n",
    "          ' qc_num: {:.2f} | ac_num: {:.2f} |'.format((node_type_ids == 0).float().sum(1).mean().item(),\n",
    "                                                      (node_type_ids == 1).float().sum(1).mean().item()))\n",
    "\n",
    "    edge_index = list(map(list, zip(*(iter(edge_index),) * num_choice))) #list of size (n_questions, n_choices), where each entry is tensor[2, E] #this operation corresponds to .view(n_questions, n_choices)\n",
    "    edge_type = list(map(list, zip(*(iter(edge_type),) * num_choice))) #list of size (n_questions, n_choices), where each entry is tensor[E, ]\n",
    "\n",
    "    concept_ids, node_type_ids, node_scores, adj_lengths = [x.view(-1, num_choice, *x.size()[1:]) for x in (concept_ids, node_type_ids, node_scores, adj_lengths)]\n",
    "    #concept_ids: (n_questions, num_choice, max_node_num)\n",
    "    #node_type_ids: (n_questions, num_choice, max_node_num)\n",
    "    #node_scores: (n_questions, num_choice, max_node_num)\n",
    "    #adj_lengths: (n_questions,　num_choice)\n",
    "    return concept_ids, node_type_ids, node_scores, adj_lengths, (edge_index, edge_type) #, half_n_rel * 2 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28e750d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM_QAGNN_DataLoader(object):\n",
    "\n",
    "    def __init__(self, args, train_statement_path, train_adj_path,\n",
    "                 dev_statement_path, dev_adj_path,\n",
    "                 test_statement_path, test_adj_path,\n",
    "                 batch_size, eval_batch_size, device, model_name, max_node_num=200, max_seq_length=128,\n",
    "                 is_inhouse=False, inhouse_train_qids_path=None,\n",
    "                 subsample=1.0, use_cache=True):\n",
    "        '''\n",
    "        params:\n",
    "            train_statement_path: train pat\n",
    "            train_adj_path: train (edge index, edge type)\n",
    "            dev and test are the same as train\n",
    "            batch_size: given size of batch\n",
    "            device: gpu or not\n",
    "            model_name: name of model\n",
    "            \n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.batch_size = batch_size\n",
    "        self.eval_batch_size = eval_batch_size\n",
    "        self.device0, self.device1 = device\n",
    "#         self.device = device\n",
    "        self.is_inhouse = is_inhouse\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        print ('train_statement_path', train_statement_path)\n",
    "        self.train_qids, self.train_labels, *self.train_encoder_data = load_tensor(train_statement_path,\n",
    "                                                                                  model_name,max_seq_length)\n",
    "        self.dev_qids, self.dev_labels, *self.dev_encoder_data = load_tensor(dev_statement_path,\n",
    "                                                                                  model_name,max_seq_length)\n",
    "        # input shape is [num_question, num_choices, dim_size]\n",
    "        self.num_choice = self.train_encoder_data[0].shape[1]\n",
    "        \n",
    "        # then load the adjacency matrix\n",
    "        *self.train_decoder_data, self.train_adj_data = load_sparse_adj_data_with_contextnode(train_adj_path,max_node_num,self.num_choice,args)\n",
    "        *self.dev_decoder_data, self.dev_adj_data = load_sparse_adj_data_with_contextnode(dev_adj_path, max_node_num, self.num_choice, args)\n",
    "        assert all(len(self.train_qids) == len(self.train_adj_data[0]) == x.size(0) for x in [self.train_labels] + self.train_encoder_data + self.train_decoder_data)\n",
    "        assert all(len(self.dev_qids) == len(self.dev_adj_data[0]) == x.size(0) for x in [self.dev_labels] + self.dev_encoder_data + self.dev_decoder_data)\n",
    "        \n",
    "        # whether use in house test\n",
    "        if self.is_inhouse:\n",
    "            with open(inhouse_train_qids_path, 'r') as fin:\n",
    "                inhouse_qids = set(line.strip() for line in fin)\n",
    "            self.inhouse_train_indexes = torch.tensor([i for i, qid in enumerate(self.train_qids) if qid in inhouse_qids])\n",
    "            self.inhouse_test_indexes = torch.tensor([i for i, qid in enumerate(self.train_qids) if qid not in inhouse_qids])\n",
    "        \n",
    "        \n",
    "        # sub sample:\n",
    "        assert 0. < subsample <= 1.\n",
    "        if subsample < 1.:\n",
    "            n_train = int(self.train_size() * subsample)\n",
    "            assert n_train > 0\n",
    "            if self.is_inhouse:\n",
    "                self.inhouse_train_indexes = self.inhouse_train_indexes[:n_train]\n",
    "            else:\n",
    "                self.train_qids = self.train_qids[:n_train]\n",
    "                self.train_labels = self.train_labels[:n_train]\n",
    "                self.train_encoder_data = [x[:n_train] for x in self.train_encoder_data]\n",
    "                self.train_decoder_data = [x[:n_train] for x in self.train_decoder_data]\n",
    "                self.train_adj_data = self.train_adj_data[:n_train]\n",
    "                assert all(len(self.train_qids) == len(self.train_adj_data[0]) == x.size(0) for x in [self.train_labels] + self.train_encoder_data + self.train_decoder_data)\n",
    "            assert self.train_size() == n_train\n",
    "    \n",
    "    \n",
    "    def train_size(self):\n",
    "        return self.inhouse_train_indexes.size(0) if self.is_inhouse else len(self.train_qids)\n",
    "\n",
    "    def dev_size(self):\n",
    "        return len(self.dev_qids)\n",
    "    \n",
    "    def train(self):\n",
    "        if self.is_inhouse:\n",
    "            n_train = self.inhouse_train_indexes.size(0)\n",
    "            train_indexes = self.inhouse_train_indexes[torch.randperm(n_train)]\n",
    "        else:\n",
    "            train_indexes = torch.randperm(len(self.train_qids))\n",
    "        return MultiGPUSparseAdjDataBatchGenerator(self.args, 'train', self.device0, self.device1, self.batch_size, train_indexes, self.train_qids, self.train_labels, tensors0=self.train_encoder_data, tensors1=self.train_decoder_data, adj_data=self.train_adj_data)\n",
    "    def dev(self):\n",
    "        return MultiGPUSparseAdjDataBatchGenerator(self.args, 'eval', self.device0, self.device1, self.batch_size, torch.arang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8e6f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiGPUSparseAdjDataBatchGenerator(object):\n",
    "    '''\n",
    "    This class include all the info that will be needed when training\n",
    "    '''\n",
    "    def __init__(self, args, mode, device0, device1, batch_size, indexes, qids, labels,\n",
    "                 tensors0=[], lists0=[], tensors1=[], lists1=[], adj_data=None):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.mode = mode\n",
    "        self.device0 = device0\n",
    "        self.device1 = device1\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = indexes\n",
    "        self.qids = qids\n",
    "        self.labels = labels\n",
    "        self.tensors0 = tensors0\n",
    "        self.lists0 = lists0\n",
    "        self.tensors1 = tensors1\n",
    "        self.lists1 = lists1\n",
    "        self.adj_data = adj_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.indexes.size(0) - 1)// self.batch_size +1\n",
    "\n",
    "    def __iter__(self):\n",
    "        '''\n",
    "        This function just like the training iterator\n",
    "        '''\n",
    "        bs = self.batch_size\n",
    "        n = self.indexes.size(0)\n",
    "        remain = n % bs\n",
    "        if remain >0:\n",
    "            extra = np.random.choice(self.indexes[:-remain], size = (bs - remain), replace = False)\n",
    "            self.indexes = torch.cat([self.indexes, torch.tensor(extra)])\n",
    "            n = self.indexes.size(0)\n",
    "        for a in range(0,n,bs):\n",
    "            b = min(n, a+bs)\n",
    "            batch_indexes = self.indexes[a:b]\n",
    "            batch_qids = [self.qids[idx] for idx in batch_indexes]\n",
    "            batch_labels = self._to_device(self.labels[batch_indexes], self.device1)\n",
    "            batch_tensors0 = [self._to_device(x[batch_indexes], self.device0) for x in self.tensors0]\n",
    "            batch_tensors1 = [self._to_device(x[batch_indexes], self.device1) for x in self.tensors1]\n",
    "            batch_lists0 = [self._to_device([x[i] for i in batch_indexes], self.device0) for x in self.lists0]\n",
    "            batch_lists1 = [self._to_device([x[i] for i in batch_indexes], self.device1) for x in self.lists1]\n",
    "            # print all LM data:\n",
    "#             print(\"batch qids\", batch_qids, len(batch_qids))\n",
    "#             print(\"batch_labels\",batch_labels, len(batch_labels))\n",
    "#             print(\"batch_tensors0\", batch_tensors0, len(batch_tensors0))\n",
    "#             print(\"batch_tensor1\", batch_tensors1, len(batch_tensors1))\n",
    "#             print(\"batch_list0\", batch_lists0, len(batch_lists0))\n",
    "#             print(\"batch_list1\", batch_lists1, len(batch_lists1))\n",
    "            edge_index_all, edge_type_all = self.adj_data\n",
    "            #edge_index_all: nested list of shape (n_samples, num_choice), where each entry is tensor[2, E]\n",
    "            #edge_type_all:  nested list of shape (n_samples, num_choice), where each entry is tensor[E, ]\n",
    "            edge_index = self._to_device([edge_index_all[i] for i in batch_indexes], self.device1)\n",
    "            edge_type  = self._to_device([edge_type_all[i] for i in batch_indexes], self.device1)\n",
    "#             print(edge_index, len(edge_index))\n",
    "#             print(edge_type, len(edge_type))\n",
    "            yield tuple([batch_qids, batch_labels, *batch_tensors0, *batch_lists0, *batch_tensors1, *batch_lists1, edge_index, edge_type])\n",
    "    def _to_device(self, obj, device):\n",
    "        if isinstance(obj, (tuple, list)):\n",
    "            return [self._to_device(item, device) for item in obj]\n",
    "        else:\n",
    "            return obj.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cac9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self, mode, num_relation, train_adj_path, dev_adj_path,train_statement_path, dev_statement_path,\n",
    "                 num_attention_head,\n",
    "                k, gnn_dim, fc_dim, num_fc_layer,freeze_,max_node_num, init_range, dropout_embd,\n",
    "                dropout_gnn, dropout_fc,model_name):\n",
    "        # for data path\n",
    "        self.mode = mode\n",
    "        self.num_relation = num_relation\n",
    "        self.train_adj_path = train_adj_path\n",
    "        self.dev_adj_path = dev_adj_path\n",
    "        self.train_statement_path = train_statement_path\n",
    "        self.dev_statement_path = dev_statement_path\n",
    "        \n",
    "        # for model\n",
    "        # language model\n",
    "        self.model_name = model_name\n",
    "        self.gnn_dim = gnn_dim\n",
    "        self.num_attention_head = num_attention_head\n",
    "        self.fc_dim = fc_dim\n",
    "        self.num_fc_layer = num_fc_layer\n",
    "        self.k = k\n",
    "        self.max_node_num = max_node_num\n",
    "        self.freeze_ = freeze_\n",
    "        self.init_range = init_range\n",
    "        \n",
    "        # for regularization\n",
    "        self.dropout_fc = dropout_fc\n",
    "        self.dropout_embd = dropout_embd\n",
    "        self.dropout_gnn = dropout_gnn\n",
    "        \n",
    "        self.max_seq_length = 128\n",
    "        self.encoder_lr = 1e-5\n",
    "        self.decoder_lr = 1e-3\n",
    "        self.batch_size = 4\n",
    "        self.load_model_path = True\n",
    "        self.load_model_path_ = \"./data/checkpoint.pt\"\n",
    "        self.weight_decay = 0.1\n",
    "        self.n_epochs = 15\n",
    "        self.cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcaf910",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(mode = 'train',num_relation=38, train_adj_path= \"./data/needed_data/train.graph.adj.pk\", \n",
    "               dev_adj_path=\"./data/needed_data/dev.graph.adj.pk\", train_statement_path=\"./data/needed_data/train.statement.jsonl\",\n",
    "               dev_statement_path=\"./data/needed_data/dev.statement.jsonl\", num_attention_head=2,\n",
    "               k=5, gnn_dim=100, fc_dim=200, num_fc_layer=0, freeze_=True, max_node_num=200, init_range=0.02,\n",
    "               dropout_embd=0.2, dropout_fc=0.2, dropout_gnn = 0.2,model_name=\"./data/needed_data/pretrained_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90c4721",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LM_QAGNN_DataLoader(config, config.train_statement_path, config.train_adj_path,\n",
    "                             config.dev_statement_path, config.dev_adj_path,None,None,\n",
    "                             batch_size=config.batch_size, eval_batch_size= config.batch_size,\n",
    "                             model_name= config.model_name,max_node_num=config.max_node_num,max_seq_length=config.max_seq_length,\n",
    "                             device = (torch.device(\"cpu\"),torch.device(\"cpu\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbbd14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    # load concept embedding\n",
    "    model_path = \"./checkpoint.pt\"\n",
    "    cp_embd = [np.load(path) for path in [\"./data/needed_data/tzw.ent.npy\"]]\n",
    "    cp_embd= torch.tensor(np.concatenate(cp_embd, 1), dtype=torch.float)\n",
    "    concept_num, concept_dim = cp_embd.shape[0], cp_embd.shape[1]\n",
    "    print('| num_concepts: {} |'.format(concept_num))\n",
    "    if torch.cuda.device_count() >= 2 and args.cuda:\n",
    "            device0 = torch.device(\"cuda:0\")\n",
    "            device1 = torch.device(\"cuda:1\")\n",
    "    elif torch.cuda.device_count() == 1 and args.cuda:\n",
    "        device0 = torch.device(\"cuda:0\")\n",
    "        device1 = torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        device0 = torch.device(\"cpu\")\n",
    "        device1 = torch.device(\"cpu\")\n",
    "    print(\"train on device \" + str(device0) + \" \" + str(device1))\n",
    "    device0 = device1 = torch.device(\"cpu\")\n",
    "    dataset = LM_QAGNN_DataLoader(config, config.train_statement_path, config.train_adj_path,\n",
    "                             config.dev_statement_path, config.dev_adj_path,None,None,\n",
    "                             batch_size=config.batch_size, eval_batch_size= config.batch_size,\n",
    "                             model_name= config.model_name,max_node_num=config.max_node_num,max_seq_length=config.max_seq_length,\n",
    "                             device = (device0,device1))\n",
    "    print(dataset.train_size()//args.batch_size)\n",
    "    model = LM_QAGNN(args=config, model_name= config.model_name, k = config.k, num_node_type= 4,\n",
    "                num_edge_type=config.num_relation, num_concept=concept_num, concept_dim=config.gnn_dim,\n",
    "                concept_in_dim=1024, n_attention_head=config.num_attention_head, fc_dim=config.fc_dim,\n",
    "                num_fc_layer=config.num_fc_layer,pretrained_concept_emb=cp_embd, drop_fc= config.dropout_fc,\n",
    "                drop_gnn = config.dropout_gnn, drop_embd = config.dropout_embd,freeze_ent_emb=False,\n",
    "                init_range=config.init_range)\n",
    "    if args.load_model_path:\n",
    "        print (f'loading and initializing model from {args.load_model_path}')\n",
    "        model_state_dict, old_args,loss_ls = torch.load(args.load_model_path_, map_location=torch.device('cpu'))\n",
    "        model.load_state_dict(model_state_dict)\n",
    "    model.encoder.to(device0)\n",
    "    model.decoder.to(device1)\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    grouped_parameters = [\n",
    "            {'params': [p for n, p in model.encoder.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay, 'lr': args.encoder_lr},\n",
    "            {'params': [p for n, p in model.encoder.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0, 'lr': args.encoder_lr},\n",
    "            {'params': [p for n, p in model.decoder.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay, 'lr': args.decoder_lr},\n",
    "            {'params': [p for n, p in model.decoder.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0, 'lr': args.decoder_lr},\n",
    "        ]\n",
    "    #set optimizer\n",
    "    optimizer = torch.optim.Adam(grouped_parameters)\n",
    "    # set warm up step\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "    max_steps = int(args.n_epochs * (dataset.train_size() / args.batch_size))\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=15, num_training_steps=max_steps)\n",
    "    print('parameters:')\n",
    "    for name, param in model.decoder.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print('\\t{:45}\\ttrainable\\t{}\\tdevice:{}'.format(name, param.size(), param.device))\n",
    "        else:\n",
    "            print('\\t{:45}\\tfixed\\t{}\\tdevice:{}'.format(name, param.size(), param.device))\n",
    "    num_params = sum(p.numel() for p in model.decoder.parameters() if p.requires_grad)\n",
    "    print('\\ttotal:', num_params)\n",
    "    # define loss\n",
    "    loss = nn.CrossEntropyLoss(reduction = \"mean\")\n",
    "    # start training\n",
    "    freeze_net(model.encoder)\n",
    "    max_norm = 1000\n",
    "    ls = []\n",
    "    # for validation\n",
    "    model.eval()\n",
    "    n_samples, n_correct = 0, 0\n",
    "    for qids, labels, *input_data in tqdm(dataset.dev()):\n",
    "        start_time = time.time()\n",
    "        logits, _ = model(*[x for x in input_data])\n",
    "        l = loss(logits, labels)\n",
    "        n_correct += (logits.argmax(1) == labels).sum().item()\n",
    "        n_samples += labels.size(0)\n",
    "    print(\"validation accuracy:\", n_correct/n_samples)\n",
    "    #------------------------------------------------ for training `------------------------------------------\n",
    "    for epoch_id in range(args.n_epochs):\n",
    "        model.train()\n",
    "        count = 0\n",
    "        for qids, labels, *input_data in dataset.train():\n",
    "            start_time = time.time()\n",
    "            optimizer.zero_grad()\n",
    "#             for a in range(0, config.batch_size, 2):\n",
    "#                 print(\"a:\", a)\n",
    "#                 b = min(a + 2, config.batch_size)\n",
    "#                 logits, _ = model(*[x[a:b] for x in input_data])\n",
    "#                 print(logits.shape)\n",
    "#                 # calculat loss\n",
    "#                 l = loss(logits, labels[a:b])\n",
    "#                 # backward\n",
    "#                 l.backward()\n",
    "#                 print(\"done one train\")\n",
    "            #[print(x, x.shape) for x in input_data]\n",
    "            logits, _ = model(*[x for x in input_data])\n",
    "#             print(logits.shape)\n",
    "            # calculat loss\n",
    "            l = loss(logits, labels)\n",
    "            # backward\n",
    "            l.backward()\n",
    "#             print(\"done one train\")\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                print(\"done train epochs \", epoch_id)\n",
    "                if count % 30 == 0 or count == dataset.train_size() - 1:\n",
    "                    ls.append(l.item())\n",
    "                    with open(\"train_log\",\"a\") as f:\n",
    "                        f.write(\"Epoch %s, Loss %s, Batch %s: %.4f sec\\n\"%(epoch_id, ls ,count, time.time() - start_time))\n",
    "                count +=1\n",
    "                #save model per each epochs\n",
    "            torch.save([model.state_dict(), args, ls], f\"{model_path}\")\n",
    "            print(\"save to \", model_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cbbd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6267863",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = torch.load(\"./checkpoint.pt\")[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
